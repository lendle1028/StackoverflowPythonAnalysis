library(keras)
max_features <- 20000
batch_size <- 32
# Cut texts after this number of words (among top max_features most common words)
maxlen <- 80
cat('Loading data...\n')
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
dim(x_train)
class(x_train)
length(x_train)
a=list(c(1,2,3), c(4,5,6))
a[[2]]
a[[1]]
a[1]
a[2]
a[3]
x_train[[1]]
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
x_train[1,]
help("layer_embedding")
model <- keras_model_sequential()
model %>%
layer_embedding(input_dim = max_features, output_dim = 128) %>%
layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Try using different optimizers and different optimizer configs
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = 15,
validation_data = list(x_test, y_test)
)
library(keras)
max_features <- 20000
batch_size <- 32
# Cut texts after this number of words (among top max_features most common words)
maxlen <- 80
cat('Loading data...\n')
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
dim(x_train)
x_train
View(x_train)
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
help("pad_sequences")
model <- keras_model_sequential()
model %>%
layer_embedding(input_dim = max_features, output_dim = 128) %>%
layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>%
layer_dense(units = 1, activation = 'sigmoid')
help("layer_embedding")
max_features
help(layer_lstm)
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = 15,
validation_data = list(x_test, y_test)
)
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = 2,
validation_data = list(x_test, y_test)
)
library(keras)
max_features <- 50000
batch_size <- 32
# Cut texts after this number of words (among top max_features most common words)
maxlen <- 50
cat('Loading data...\n')
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')
cat('Pad sequences (samples x time)\n')
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')
x_train=x_train[1:5000,]
cat('Build model...\n')
model <- keras_model_sequential()
model %>%
layer_embedding(input_dim = max_features, output_dim = 128) %>%
layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Try using different optimizers and different optimizer configs
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
cat('Train...\n')
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = 15,
validation_data = list(x_test, y_test)
)
scores <- model %>% evaluate(
x_test, y_test,
batch_size = batch_size
)
library(keras)
max_features <- 50000
batch_size <- 32
# Cut texts after this number of words (among top max_features most common words)
maxlen <- 50
cat('Loading data...\n')
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')
cat('Pad sequences (samples x time)\n')
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')
x_train=x_train[1:5000,]
y_train=y_train[1:5000,]
cat('Build model...\n')
model <- keras_model_sequential()
model %>%
layer_embedding(input_dim = max_features, output_dim = 128) %>%
layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Try using different optimizers and different optimizer configs
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
cat('Train...\n')
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = 15,
validation_data = list(x_test, y_test)
)
scores <- model %>% evaluate(
x_test, y_test,
batch_size = batch_size
)
x_train
y_train[1:5000]
library(keras)
max_features <- 50000
batch_size <- 32
# Cut texts after this number of words (among top max_features most common words)
maxlen <- 50
cat('Loading data...\n')
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')
cat('Pad sequences (samples x time)\n')
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')
x_train=x_train[1:5000,]
y_train=y_train[1:5000]
cat('Build model...\n')
model <- keras_model_sequential()
model %>%
layer_embedding(input_dim = max_features, output_dim = 128) %>%
layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Try using different optimizers and different optimizer configs
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
cat('Train...\n')
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = 15,
validation_data = list(x_test, y_test)
)
library(keras)
max_features <- 50000
batch_size <- 64
# Cut texts after this number of words (among top max_features most common words)
maxlen <- 50
cat('Loading data...\n')
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')
cat('Pad sequences (samples x time)\n')
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')
x_train=x_train[1:5000,]
y_train=y_train[1:5000]
cat('Build model...\n')
model <- keras_model_sequential()
model %>%
layer_embedding(input_dim = max_features, output_dim = 256) %>%
layer_lstm(units = 128, dropout = 0.25, recurrent_dropout = 0.25) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Try using different optimizers and different optimizer configs
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
cat('Train...\n')
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = 5,
validation_data = list(x_test, y_test)
)
scores <- model %>% evaluate(
x_test, y_test,
batch_size = batch_size
)
cat('Test score:', scores[[1]])
cat('Test accuracy', scores[[2]])
R.version
R.version.string
setwd("~/dev/NetBeansProjects/LabUtils/StackoverflowPythonAnalysis/python_questions")
load(file='.RData')
View(d1)
colSums(d1)
class(colSums(d1))
colSums(d1)['django']
colSums(d1)
m=colSums(d1)
which(m<100)
m['java']
m['javascript']
m['opencv']
miin(m)
min(m)
m=colSums(d)
min(m)
m=colSums(d1)
max(m)
min(d$java)
max(d$java)
quantile(m)
help(quantile)
quantile(m, probs=seq(0,1, 0.1))
m=colSums(d1[,2:565])
quantile(m, probs=seq(0,1, 0.1))
which(m<100)
which(m<200)
length(which(m<200))
names(d)[290]
names(d)[291]
m+1
which(m<100)
which(m<=100)
which(m<100)
M1=which(m<100)
rm(M1)
m1=which(m<100)
m1+1
d[,m1+1]
m1=which(m<=100)
d[,m1+1]
colnames(d)
colnames(d)[m+1]
colnames(d)[m]
m1=which(m<100)
colnames(d)[m+1]
colnames(d)
m1
m1+1
colnames(d)[1:2]
colnames(d)[as.vector(m1+1)]
m1=~which(m<=100)
length(m1)
quantile(m)
m=colSums(d1[,2:565])
quantile(m)
which(m>100)
length(which(m>100))
which(m>5000)
m1=(which(m>100))
d2=d1[,m1]
d2=d1[,m1+1]
rowSums(d2[2:468])
which(rowSums(d2[2:468])==0)
m2=(rowSums(d2[2:468]))
d2$rowSums=m2
d2[d2$rowSums==0,]
nrow(d2[d2$rowSums==0,])
d1=d2
d=d[,m1+1]
d$rowSums=rowSums(d[,2:468])
nrow(d[d$rowSums==0,])
d=d[d$rowSums>0,]
d1=d2[d2$rowSums>0,]
rm(d2)
rm(m2)
m1=colSums(d1[,2:469])
sort(m1)
mean(d[d$numpy>0,]$numpy)
mean(d[d$python>0,]$python)
mean(d[d$pandas>0,]$pandas)
mean(d[d$opencv>0,]$opencv)
mean(d[d$sorting>0,]$sorting)
mean(d[d$pip>0,]$pip)
mean(d[d$linux>0,]$linux)
mean(d[d$string>0,]$string)
mean(d[d$range>0,]$range)
d$rowSums=NULL
d1$rowSums=NULL
apply(names(d), 1, print)
apply(names(d), 2, print)
names(d)
as.vector(names(d))
apply(as.vector(names(d)), 2, print)
dim(as.vector(names(d)))
dim(as.list(names(d)))
dim(names(d))
class(names(d))
vector(names(d))
c(names(d))
dim(c(names(d)))
lapply(as.vector(names(d)), print)
lapply(as.list(names(d)), print)
sapply(as.list(names(d)), print)
getKeywordPosition<-func(df, key){mean(df[df[,key]>0,key]);}
getKeywordPosition<-function(df, key){mean(df[df[,key]>0,key]);}
getKeywordPosition(d, "python")
getKeywordPosition(d, "opencv")
getKeywordPosition2<-function(key){mean(d[d[,key]>0,key]);}
getKeywordPosition2("opencv")
sapply(as.vector(names(d)), getKeywordPosition2)
m1=sapply(as.vector(names(d)), getKeywordPosition2)
which(m1<=2)
length(which(m1<=2))
length(which(m1<2))
d$lua
d[d$lua>0,]$lua
d[d$opencv>0,]$opencv
which(m1<2)
m1['opencv']
View(getKeywordPosition)
mean(d[d$opencv>0,]$opencv)
min(d[d$opencv>0,]$opencv)
min(d$opencv)
max(d$opencv)
unique(d$opencv)
d[d$opencv==1,]
nrow(d[d$opencv==1,])
d[d$opencv==1,]$python
nrow(d[d$python==0,])
nrow(d[d$python==1,])
nrow(d[d$python==2,])
nrow(d[d$python>1,])
d=d[d$python==0,]
d1=d1[d$python==0,]
d$python=1
d[d>0,]
save.image()
d=read.csv('result.csv')
d1=read.csv('result0or1.csv')
m=colSums(d[,2:565])
quantile(m, probs = seq(0,1,0.1))
m1=which(m>200)
length(m1)
d2=d1[,c(1,m1+1)]
d1=d2
rm(d2)
names(d1)
d=d[,c(1, m+1)]
d=d[,c(1, m1+1)]
d$rowSums=rowSums(d[,2:473])
d1$rowSums=rowSums(d1[,2:473])
nrow(d[d$rowSums>0,])
quantile(d1$rowSums)
nrow(d1[d1$rowSums>1,])
sapply(names(d)[2:473], getKeywordPosition2)
m1=sapply(names(d)[2:473], getKeywordPosition2)
m1['python']
which(m1<=2)
quantile(m1)
m1['java']
m1['javascript']
d[d$javascript==1,]
d[d$javascript==1,]$python
nrow(d[d$python==0,])
d=d[d$python>0,]
d1=d1[d1$python>0,]
m1=sapply(names(d)[2:473], getKeywordPosition2)
which(m1<=2)
m1['java']
nrow(d[d$python==0,])
d[d$java==1,]$python
d=d[d$python==1,]
m1=sapply(names(d)[2:473], getKeywordPosition2)
m1['java']
which(m1<=2)
m1['json']
m1['pandas']
m1['numpy']
m1['tensorflow']
nrow(d[d$pandas>0,])
nrow(d[d$tensorflow>0,])
quantile(d[d$tensorflow>0,]$tensorflow)
nrow(d[d$tensorflow==2,])
head(d[d$tensorflow==2,])
head(d[d$tensorflow==3,])
d=read.csv('result.csv')
d1=read.csv('result0or1.csv')
d$id
d[d$id==36382508,]
nrow(d[d$id==36382508,])
d=read.csv('result.csv')
nrow(d[d$id==36382508,])
d$id
d=read.csv('result.csv')
d$id
nrow(d[d$id==36382508,])
d1=read.csv('result0or1.csv')
m=colSums(d1[,2:565])
quantile(m, probs=seq(0,1,0.1))
m1=which(m>18)
length(m1)
d2=d1[,m1+1]
d2=d1[,c(1,m1+1)]
d1=d2
rm(d2)
d=d[,c(1,m1+1)]
d$rowSums=rowSums(d[2:392])
d1$rowSums=rowSums(d1[2:392])
nrow(d[d$rowSums==0,])
nrow(d1[d1$rowSums==0,])
nrow(d1[d1$rowSums==1,])
d=d[d$rowSums>1,]
d1=d1[d1$rowSums>1,]
d$rowSums=NULL
d1$rowSums=NULL
d=d[d$python==1,]
sapply(d[,2:392], getKeywordPosition2)
sapply(names(d)[2:392], getKeywordPosition2)
m1=sapply(names(d)[2:392], getKeywordPosition2)
which(m1<=2)
which(m1<3)
sort(which(m1<3))
sort(which(m1<3))['exception']
m1['exception']
save.image()
m1['tensorflow']
which(m1<2.5)
quantile(m1)
m1
which(m1<2.2)
